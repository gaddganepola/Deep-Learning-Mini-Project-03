{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOMvnejHShL6VjYcN1kd2Fn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## **English to Sinhala Translation with Transformers**"],"metadata":{"id":"z62oiwR7EOO7"}},{"cell_type":"markdown","source":["## **Necessary Library Imports**"],"metadata":{"id":"ypyTCXnZEYBL"}},{"cell_type":"code","source":["import random\n","import tensorflow as tf\n","import string\n","import re\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"id":"ZFhEeiEdBMiG","executionInfo":{"status":"ok","timestamp":1713693415773,"user_tz":-330,"elapsed":4464,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## **Prepare the Data**"],"metadata":{"id":"064n3chXB1Aw"}},{"cell_type":"markdown","source":["## **Mount the Google Drive**"],"metadata":{"id":"PfOyBc9YCmHX"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6Ic4Nx5CpIm","executionInfo":{"status":"ok","timestamp":1713693437018,"user_tz":-330,"elapsed":18836,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}},"outputId":"4b78b03b-9d43-41b9-be8e-12db7db79985"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## **Read the data file**"],"metadata":{"id":"7N3mJOsICxM2"}},{"cell_type":"code","source":["text_file = \"/content/drive/My Drive/dataset/EnglishSinhalaDataset.txt\"\n","with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","i = 0\n","for line in lines:\n","  print(line)\n","  i = i + 1\n","  if(i==20):\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r54XbpqFC1YF","executionInfo":{"status":"ok","timestamp":1713693443383,"user_tz":-330,"elapsed":1246,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}},"outputId":"eec08e39-4168-4ef8-e42d-348d071e6287"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Go.\tයන්න.\n","Hi.\tආයුබෝවන්.\n","Run.\tදුවන්න.\n","Who?\tකව්ද?\n","Wow!\tවාව්!\n","Fire.\tගින්නක්.\n","Help.\tඋදව්.\n","Hide.\tසඟවන්න.\n","Jump.\tපනින්න.\n","Stay.\tරැඳී සිටින්න.\n","Stop.\tනවත්වන්න.\n","Wait.\tඉන්න.\n","Begin.\tආරම්භය.\n","Go on.\tදිගටම යන්න.\n","Hello!\tහෙලෝ!\n","Hurry!\tඉක්මන් කරන්න!\n","I hid.\tමම සැඟවී සිටියෙමි.\n","I ran.\tමම දිව්වා.\n","I try.\tමම උත්සාහ කරමි.\n","I won.\tමම දිනුවා.\n"]}]},{"cell_type":"code","source":["for x in range(len(lines)-10,len(lines)):\n","  print(lines[x])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQ9WEX79Dah_","executionInfo":{"status":"ok","timestamp":1713693446295,"user_tz":-330,"elapsed":8,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}},"outputId":"dc6591f5-cf22-4f81-bae5-e51c29821eef"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\tචොක්ලට් යනු වැනිලා බව මිනිසුන්ට ඒත්තු ගැන්වීමට ඔබ කොතරම් උත්සාහ කළත්, එය වැනිලා බව ඔබට සහ තවත් කිහිප දෙනෙකුට ඒත්තු ගැන්විය හැකි වුවද, එය තවමත් චොකලට් වනු ඇත.\n","In 1969, Roger Miller recorded a song called \"You Don't Want My Love.\" Today, this song is better known as \"In the Summer Time.\" It's the first song he wrote and sang that became popular.\t1969 දී රොජර් මිලර් \"ඔබට මගේ ආදරය අවශ්‍ය නැත\" නමින් ගීතයක් පටිගත කළේය. අද මෙම ගීතය වඩාත් ප්‍රචලිත වන්නේ \"ගිම්හානයේ\" යනුවෙනි. එය ඔහු ලියූ සහ ගායනා කළ පළමු ගීතය ජනප්‍රිය විය.\n","A child who is a native speaker usually knows many things about his or her language that a non-native speaker who has been studying for years still does not know and perhaps will never know.\tස්වදේශික කථිකයෙකු වන දරුවෙකු සාමාන්‍යයෙන් ඔහුගේ හෝ ඇයගේ භාෂාව පිළිබඳ බොහෝ දේ දන්නා අතර එය වසර ගණනාවක් තිස්සේ අධ්‍යයනය කරන ස්වදේශික නොවන කථිකයෙකු තවමත් නොදන්නා සහ කිසි විටෙකත් නොදැන සිටිය හැකිය.\n","There are four main causes of alcohol-related death. Injury from car accidents or violence is one. Diseases like cirrhosis of the liver, cancer, heart and blood system diseases are the others.\tමත්පැන් නිසා සිදුවන මරණවලට ප්‍රධාන හේතු හතරක් තිබේ. රිය අනතුරකින් හෝ ප්‍රචණ්ඩත්වයකින් තුවාල වීම එකකි. අක්මාවේ සිරෝසිස්, පිළිකා, හෘද රෝග සහ රුධිර සංසරණ පද්ධතිය වැනි රෝග අනෙක් ඒවා වේ.\n","There are mothers and fathers who will lie awake after the children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for their child's college education.\tඋකස් හෝ දොස්තරගේ බිල් ගෙවන්නේ කෙසේද, දරුවන්ගේ විශ්වවිද්‍යාල අධ්‍යාපනයට ප්‍රමාණවත් මුදලක් ඉතිරි කරන්නේ කෙසේදැයි කල්පනා කරමින් දරුවන් නිදාගත් පසු අවදියෙන් සිටින අම්මලා තාත්තලාද සිටිති.\n","A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climate change.\tකාබන් පියසටහනක් යනු අපගේ ක්‍රියාකාරකම්වල ප්‍රතිඵලයක් ලෙස අප විසින් නිපදවන කාබන්ඩයොක්සයිඩ් දූෂණ ප්‍රමාණයයි. සමහර අය දේශගුණික විපර්යාස ගැන සැලකිලිමත් වන නිසා ඔවුන්ගේ කාබන් පියසටහන අඩු කිරීමට උත්සාහ කරති.\n","Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.\tඕනෑම මාතෘකාවක සාමාන්‍යයෙන් වෙබ් පිටු කිහිපයක් ඇති බැවින්, මම සාමාන්‍යයෙන් උත්පතන දැන්වීම් ඇති වෙබ් පිටුවකට පැමිණෙන විට ආපසු බොත්තම ඔබන්නෙමි. මම සරලවම Google විසින් සොයා ගන්නා ලද ඊළඟ පිටුවට ගොස් අඩු කෝපයක් ඇති දෙයක් සොයා ගැනීමට බලාපොරොත්තු වෙමි.\n","If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\tඔබට ස්වදේශික කථිකයෙකු ලෙස ශබ්ද කිරීමට අවශ්‍ය නම්, බැන්ජෝ වාදකයෙකු එම වාක්‍ය ඛණ්ඩය නිවැරදිව හා නියමිත වේලාවට වාදනය කරන තෙක් එකම වාක්‍ය ඛණ්ඩය නැවත නැවතත් කීමට පුහුණු වීමට ඔබ කැමති විය යුතුය.\n","It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.\tමෙම ආකාරයේ සහයෝගී ප්‍රයත්නයේ ස්වභාවය නිසා සම්පූර්ණ දෝෂ රහිත corpus එකක් ලබා ගැනීමට නොහැකි විය හැක. කෙසේ වෙතත්, ඔවුන් ඉගෙන ගන්නා භාෂා සමඟ අත්හදා බැලීම් කරනවාට වඩා ඔවුන්ගේම භාෂාවෙන් වාක්‍ය ඛණ්ඩ දායක කිරීමට අපි සාමාජිකයින් දිරිමත් කරන්නේ නම්, අපට දෝෂ අවම කර ගැනීමට හැකි වනු ඇත.\n","One day, I woke up to find that God had put hair on my face. I shaved it off. The next day, I found that God had put it back on my face, so I shaved it off again. On the third day, when I found that God had put hair back on my face again, I decided to let God have his way. That's why I have a beard.\tදවසක් මම ඇහැරිලා බැලුවා දෙවියන් මගේ මූණට කෙස් ගහලා කියලා. මම ඒක රැවුල කැපුවා. ඊළඟ දවසේ, දෙවියන් වහන්සේ එය මගේ මුහුණට දමා ඇති බව මම දුටුවෙමි, මම එය නැවත රැවුල කපා ගත්තෙමි. තුන්වෙනි දවසේදී, දෙවියන් වහන්සේ නැවතත් මගේ මුහුණට හිසකෙස් දමා ඇති බව මම දුටු විට, මම දෙවියන් වහන්සේට ඔහුගේ මාර්ගයට ඉඩ දීමට තීරණය කළෙමි. ඒකයි මට රැවුල තියෙන්නේ.\n"]}]},{"cell_type":"markdown","source":["## **Split the English and Sinhala translation pairs**"],"metadata":{"id":"YOe6xt5DDiwt"}},{"cell_type":"code","source":["text_pairs = []\n","for line in lines:\n","    english, sinhala = line.split(\"\\t\")\n","    sinhala = \"[start] \" + sinhala + \" [end]\"\n","    text_pairs.append((english, sinhala))\n","for i in range(3):\n","  print(random.choice(text_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cMbEoERuDofd","executionInfo":{"status":"ok","timestamp":1713693449461,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}},"outputId":"514d8dcd-2d7c-4106-f504-5928d888a6ea"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["('My stepfather was diabetic.', '[start] මගේ සුළු පියා දියවැඩියා රෝගියෙක්. [end]')\n","('I wonder if this rumor is true.', '[start] මේ කටකතාව ඇත්තක්ද කියලා මට පුදුමයි. [end]')\n","('I went to bed a little later than usual.', '[start] මම වෙනදට වඩා ටිකක් වෙලා නිදාගන්න ගියා. [end]')\n"]}]},{"cell_type":"markdown","source":["## **Randomize the data**"],"metadata":{"id":"WJXncxGmEFD_"}},{"cell_type":"code","source":["import random\n","random.shuffle(text_pairs)"],"metadata":{"id":"LGHdkSQAELpW","executionInfo":{"status":"ok","timestamp":1713693451879,"user_tz":-330,"elapsed":742,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## **Spliting the data into training, validation and Testing**"],"metadata":{"id":"GV_IxVwqEeAN"}},{"cell_type":"code","source":["num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples:]\n","print(\"Total sentences:\",len(text_pairs))\n","print(\"Training set size:\",len(train_pairs))\n","print(\"Validation set size:\",len(val_pairs))\n","print(\"Testing set size:\",len(test_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mn1O9uO8Ef-u","executionInfo":{"status":"ok","timestamp":1713693454295,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}},"outputId":"4ba40c6e-5c7e-4c6c-e02a-95681b4033fd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Total sentences: 125603\n","Training set size: 87923\n","Validation set size: 18840\n","Testing set size: 18840\n"]}]},{"cell_type":"code","source":["len(train_pairs)+len(val_pairs)+len(test_pairs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9xrkToXE4OU","executionInfo":{"status":"ok","timestamp":1713693458994,"user_tz":-330,"elapsed":493,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}},"outputId":"96f496a5-8883-46e1-8a04-f509ca73f1db"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["125603"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## **Removing Punctuations**"],"metadata":{"id":"s3lk2MCjE8cs"}},{"cell_type":"code","source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")"],"metadata":{"id":"oqJmJK9JE-Il","executionInfo":{"status":"ok","timestamp":1713693461072,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["f\"[{re.escape(strip_chars)}]\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"XTxqVaGNFcEV","executionInfo":{"status":"ok","timestamp":1713693463684,"user_tz":-330,"elapsed":10,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}},"outputId":"9fb211a4-9dc1-48fe-ad75-4866c6145830"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["f\"{3+5}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"XiLo-J9XFjyW","executionInfo":{"status":"ok","timestamp":1713693465578,"user_tz":-330,"elapsed":8,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}},"outputId":"27ee3e8a-263a-4e5a-e771-4c74f08f6046"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'8'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["## **Vectorizing the English and Sinhala text pairs**"],"metadata":{"id":"s2_-BGmMGGsd"}},{"cell_type":"code","source":["def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(\n","        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","vocab_size = 15000\n","sequence_length = 20\n","source_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","target_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_english_texts = [pair[0] for pair in train_pairs]\n","train_sinhala_texts = [pair[1] for pair in train_pairs]\n","source_vectorization.adapt(train_english_texts)\n","target_vectorization.adapt(train_sinhala_texts)"],"metadata":{"id":"ndYTPDcZGKXe","executionInfo":{"status":"ok","timestamp":1713693501445,"user_tz":-330,"elapsed":18391,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## **Preparing datasets for the translation task**"],"metadata":{"id":"J-D2dvG3HAoF"}},{"cell_type":"code","source":["batch_size = 64\n","def format_dataset(eng, spa):\n","    eng = source_vectorization(eng)\n","    spa = target_vectorization(spa)\n","    return ({\n","        \"english\": eng,\n","        \"sinhala\": spa[:, :-1],\n","    }, spa[:, 1:])\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)\n","for inputs, targets in train_ds.take(1):\n","    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n","    print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n","    print(f\"targets.shape: {targets.shape}\")\n","inputs['english'].shape: (64, 20)\n","inputs['sinhala'].shape: (64, 20)\n","targets.shape: (64, 20)\n","print(list(train_ds.as_numpy_iterator())[50])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZkLCvFvHDwm","executionInfo":{"status":"ok","timestamp":1713693538233,"user_tz":-330,"elapsed":2855,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}},"outputId":"0ce863ae-690e-46cd-fb40-b4bb51c06192"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs['english'].shape: (64, 20)\n","inputs['sinhala'].shape: (64, 20)\n","targets.shape: (64, 20)\n","({'english': array([[   3, 1690,    5, ...,    0,    0,    0],\n","       [  24,    8,    2, ...,    0,    0,    0],\n","       [  18,    5,  450, ...,    0,    0,    0],\n","       ...,\n","       [  17,  901,   39, ...,    0,    0,    0],\n","       [  29,   35, 3226, ...,    0,    0,    0],\n","       [  28,  304,  219, ...,    0,    0,    0]]), 'sinhala': array([[    2,     4,    55, ...,     0,     0,     0],\n","       [    2,    21, 12935, ...,     0,     0,     0],\n","       [    2,     7,   150, ...,     0,     0,     0],\n","       ...,\n","       [    2,    40,  6374, ...,     0,     0,     0],\n","       [    2,     4,   565, ...,     0,     0,     0],\n","       [    2,   282,   915, ...,     0,     0,     0]])}, array([[    4,    55,     7, ...,     0,     0,     0],\n","       [   21, 12935,  9333, ...,     0,     0,     0],\n","       [    7,   150,   229, ...,     0,     0,     0],\n","       ...,\n","       [   40,  6374,  2916, ...,     0,     0,     0],\n","       [    4,   565,   676, ...,     0,     0,     0],\n","       [  282,   915,    62, ...,     0,     0,     0]]))\n"]}]},{"cell_type":"markdown","source":["## **Transformer encoder implemented as a subclassed Layer**"],"metadata":{"id":"754Mk_alIKtk"}},{"cell_type":"code","source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            mask = mask[:, tf.newaxis, :]\n","        attention_output = self.attention(\n","            inputs, inputs, attention_mask=mask)\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config"],"metadata":{"id":"7fl2JUkuIMSt","executionInfo":{"status":"ok","timestamp":1713693564306,"user_tz":-330,"elapsed":585,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## **The Transformer decoder**"],"metadata":{"id":"4Rks-e1pIlto"}},{"cell_type":"code","source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1),\n","             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n","        return tf.tile(mask, mult)\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(\n","                mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","        else:\n","            padding_mask = mask\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=causal_mask)\n","        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","        attention_output_2 = self.attention_2(\n","            query=attention_output_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        attention_output_2 = self.layernorm_2(\n","            attention_output_1 + attention_output_2)\n","        proj_output = self.dense_proj(attention_output_2)\n","        return self.layernorm_3(attention_output_2 + proj_output)"],"metadata":{"id":"GKZ0qPReIn1m","executionInfo":{"status":"ok","timestamp":1713693581228,"user_tz":-330,"elapsed":763,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## **Positional Encoding**"],"metadata":{"id":"kpWtz18nI0M9"}},{"cell_type":"code","source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=input_dim, output_dim=output_dim)\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","    def get_config(self):\n","        config = super(PositionalEmbedding, self).get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config"],"metadata":{"id":"MgSk3GqxI27-","executionInfo":{"status":"ok","timestamp":1713693601915,"user_tz":-330,"elapsed":691,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## **End-to-End Transformer**"],"metadata":{"id":"WFq5UXB8I6Dl"}},{"cell_type":"code","source":["embed_dim = 256\n","dense_dim = 2048\n","num_heads = 8\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","transformer.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTUgX6W5I9Sl","executionInfo":{"status":"ok","timestamp":1713693625613,"user_tz":-330,"elapsed":1503,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}},"outputId":"445aaae5-2969-406c-a7e4-222fe6f4b6e7"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," english (InputLayer)        [(None, None)]               0         []                            \n","                                                                                                  \n"," sinhala (InputLayer)        [(None, None)]               0         []                            \n","                                                                                                  \n"," positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n"," tionalEmbedding)                                                                                 \n","                                                                                                  \n"," positional_embedding_1 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n"," sitionalEmbedding)                                                                               \n","                                                                                                  \n"," transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n"," formerEncoder)                                                                                   \n","                                                                                                  \n"," transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n"," formerDecoder)                                                     ',                            \n","                                                                     'transformer_encoder[0][0]'] \n","                                                                                                  \n"," dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n","                                                                                                  \n"," dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 19960216 (76.14 MB)\n","Trainable params: 19960216 (76.14 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## **Training the sequence-to-sequence Transformer**"],"metadata":{"id":"6hvByzGqJTes"}},{"cell_type":"code","source":["transformer.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","transformer.fit(train_ds, epochs=50, validation_data=val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2vhe7gIIJWzX","outputId":"3b979e3d-097d-423e-c410-73fe575bf506","executionInfo":{"status":"ok","timestamp":1713699437673,"user_tz":-330,"elapsed":5763919,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","1374/1374 [==============================] - 113s 74ms/step - loss: 4.3450 - accuracy: 0.4097 - val_loss: 3.4323 - val_accuracy: 0.4951\n","Epoch 2/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 3.3913 - accuracy: 0.5027 - val_loss: 3.0765 - val_accuracy: 0.5361\n","Epoch 3/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 3.0955 - accuracy: 0.5395 - val_loss: 2.9211 - val_accuracy: 0.5592\n","Epoch 4/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.9397 - accuracy: 0.5630 - val_loss: 2.8652 - val_accuracy: 0.5706\n","Epoch 5/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.8405 - accuracy: 0.5796 - val_loss: 2.8415 - val_accuracy: 0.5778\n","Epoch 6/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.7729 - accuracy: 0.5919 - val_loss: 2.8367 - val_accuracy: 0.5814\n","Epoch 7/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.7145 - accuracy: 0.6023 - val_loss: 2.8101 - val_accuracy: 0.5862\n","Epoch 8/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.6607 - accuracy: 0.6118 - val_loss: 2.8384 - val_accuracy: 0.5867\n","Epoch 9/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.6112 - accuracy: 0.6202 - val_loss: 2.8239 - val_accuracy: 0.5896\n","Epoch 10/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.5667 - accuracy: 0.6273 - val_loss: 2.8257 - val_accuracy: 0.5926\n","Epoch 11/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.5301 - accuracy: 0.6340 - val_loss: 2.8265 - val_accuracy: 0.5935\n","Epoch 12/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.4918 - accuracy: 0.6405 - val_loss: 2.8376 - val_accuracy: 0.5957\n","Epoch 13/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.4605 - accuracy: 0.6458 - val_loss: 2.8325 - val_accuracy: 0.5976\n","Epoch 14/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.4285 - accuracy: 0.6513 - val_loss: 2.8394 - val_accuracy: 0.5965\n","Epoch 15/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.4012 - accuracy: 0.6559 - val_loss: 2.8520 - val_accuracy: 0.5987\n","Epoch 16/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.3782 - accuracy: 0.6600 - val_loss: 2.8424 - val_accuracy: 0.6006\n","Epoch 17/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.3524 - accuracy: 0.6648 - val_loss: 2.8845 - val_accuracy: 0.5960\n","Epoch 18/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.3279 - accuracy: 0.6688 - val_loss: 2.8673 - val_accuracy: 0.6028\n","Epoch 19/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.3060 - accuracy: 0.6724 - val_loss: 2.8819 - val_accuracy: 0.6022\n","Epoch 20/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.2797 - accuracy: 0.6763 - val_loss: 2.8774 - val_accuracy: 0.6031\n","Epoch 21/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.2588 - accuracy: 0.6802 - val_loss: 2.9257 - val_accuracy: 0.6012\n","Epoch 22/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.2341 - accuracy: 0.6845 - val_loss: 2.9041 - val_accuracy: 0.6051\n","Epoch 23/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.2143 - accuracy: 0.6875 - val_loss: 2.9318 - val_accuracy: 0.6053\n","Epoch 24/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.1899 - accuracy: 0.6914 - val_loss: 2.9766 - val_accuracy: 0.6017\n","Epoch 25/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.1738 - accuracy: 0.6940 - val_loss: 2.9463 - val_accuracy: 0.6034\n","Epoch 26/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.1525 - accuracy: 0.6980 - val_loss: 2.9542 - val_accuracy: 0.6068\n","Epoch 27/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.1347 - accuracy: 0.7011 - val_loss: 2.9985 - val_accuracy: 0.6035\n","Epoch 28/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.1150 - accuracy: 0.7037 - val_loss: 2.9967 - val_accuracy: 0.6068\n","Epoch 29/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.0953 - accuracy: 0.7074 - val_loss: 3.0107 - val_accuracy: 0.6092\n","Epoch 30/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.0752 - accuracy: 0.7106 - val_loss: 3.0039 - val_accuracy: 0.6082\n","Epoch 31/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 2.0546 - accuracy: 0.7132 - val_loss: 3.0153 - val_accuracy: 0.6094\n","Epoch 32/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.0366 - accuracy: 0.7166 - val_loss: 3.0285 - val_accuracy: 0.6068\n","Epoch 33/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 2.0219 - accuracy: 0.7193 - val_loss: 3.0431 - val_accuracy: 0.6096\n","Epoch 34/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 2.0040 - accuracy: 0.7216 - val_loss: 3.0429 - val_accuracy: 0.6116\n","Epoch 35/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 1.9882 - accuracy: 0.7244 - val_loss: 3.0867 - val_accuracy: 0.6105\n","Epoch 36/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 1.9732 - accuracy: 0.7265 - val_loss: 3.0935 - val_accuracy: 0.6076\n","Epoch 37/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 1.9558 - accuracy: 0.7297 - val_loss: 3.1078 - val_accuracy: 0.6105\n","Epoch 38/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.9432 - accuracy: 0.7318 - val_loss: 3.0810 - val_accuracy: 0.6123\n","Epoch 39/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 1.9255 - accuracy: 0.7348 - val_loss: 3.1354 - val_accuracy: 0.6102\n","Epoch 40/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.9098 - accuracy: 0.7374 - val_loss: 3.1707 - val_accuracy: 0.6121\n","Epoch 41/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 1.8952 - accuracy: 0.7395 - val_loss: 3.1728 - val_accuracy: 0.6093\n","Epoch 42/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 1.8796 - accuracy: 0.7420 - val_loss: 3.1721 - val_accuracy: 0.6127\n","Epoch 43/50\n","1374/1374 [==============================] - 95s 69ms/step - loss: 1.8622 - accuracy: 0.7450 - val_loss: 3.2019 - val_accuracy: 0.6099\n","Epoch 44/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 1.8532 - accuracy: 0.7466 - val_loss: 3.2263 - val_accuracy: 0.6097\n","Epoch 45/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 1.8397 - accuracy: 0.7490 - val_loss: 3.2380 - val_accuracy: 0.6120\n","Epoch 46/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 1.8288 - accuracy: 0.7507 - val_loss: 3.2208 - val_accuracy: 0.6119\n","Epoch 47/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 1.8139 - accuracy: 0.7529 - val_loss: 3.2705 - val_accuracy: 0.6123\n","Epoch 48/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 1.8027 - accuracy: 0.7548 - val_loss: 3.2507 - val_accuracy: 0.6153\n","Epoch 49/50\n","1374/1374 [==============================] - 94s 68ms/step - loss: 1.7881 - accuracy: 0.7571 - val_loss: 3.2627 - val_accuracy: 0.6159\n","Epoch 50/50\n","1374/1374 [==============================] - 94s 69ms/step - loss: 1.7773 - accuracy: 0.7589 - val_loss: 3.2581 - val_accuracy: 0.6154\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7d6aa44c7d00>"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["## **Testing**"],"metadata":{"id":"uqQRctjwe95Q"}},{"cell_type":"code","source":["import numpy as np\n","spa_vocab = target_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization(\n","            [decoded_sentence])[:, :-1]\n","        predictions = transformer(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(20):\n","    input_sentence = random.choice(test_eng_texts)\n","    print(\"-\")\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SK6fkyjpZYkU","executionInfo":{"status":"ok","timestamp":1713699453640,"user_tz":-330,"elapsed":8855,"user":{"displayName":"Dinethra Dayan","userId":"04925100266108766964"}},"outputId":"b5a2ca6e-62d6-4367-8f45-43e4047b42fd"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["-\n","Tom went to Boston at the end of October.\n","[start] ටොම් ඔක්තෝබර් මාසයේ මේරි සමඟ බොස්ටන් ගියා [end]\n","-\n","Tom threw something at me and I ducked.\n","[start] ටොම් මට මා වෙනුවෙන් යමක් රැගෙන ගියේය [end]\n","-\n","Tom certainly is clumsy.\n","[start] ටොම් ඇත්තටම [UNK] නෑ [end]\n","-\n","Tom is down with the flu.\n","[start] ටොම් උණ වට්ටනවා [end]\n","-\n","I think it's time for me to retire.\n","[start] මම හිතන්නේ මට විදේශ ගමන් බලපත්‍රය කල් [end]\n","-\n","If he had been a bird, he could have flown to you.\n","[start] ඔහු කුරුල්ලා වෙත ගොස් ඇති බව ඔහුට කුරුල්ලා දැන ගැනීමට සිදු විය [end]\n","-\n","This is not okay.\n","[start] මේක [UNK] නැහැ [end]\n","-\n","We aren't very hungry yet.\n","[start] අපිට තවම බඩගිනි නැහැ [end]\n","-\n","He tries.\n","[start] ඔහු බර උත්සාහ කරයි [end]\n","-\n","The earthquake reduced many villages to rubble.\n","[start] [UNK] ගණනාවක් [UNK] සම්පූර්ණයෙන්ම කරදර විය [end]\n","-\n","Would you please stop talking?\n","[start] කරුණාකර ඔබට නතර කිරීම නතර කළ හැකිද [end]\n","-\n","A swarm of mosquitoes followed him.\n","[start] [UNK] ඔහු පසුපස යාමට බොහෝ [UNK] [end]\n","-\n","He was right after all.\n","[start] එය සියල්ල අවසන් වූ පසු විය [end]\n","-\n","Tom should've told Mary.\n","[start] ටොම් මේරිට කියන්න තිබුණා [end]\n","-\n","Applicants are requested to apply in person.\n","[start] [UNK] ඉල්ලන ඕනෑම කෙනෙකුට එය අගය කරයි [end]\n","-\n","My sister is very intelligent.\n","[start] මගේ සහෝදරිය ඉතා දක්ෂයි [end]\n","-\n","This saying became popular overnight.\n","[start] මේක [UNK] [UNK] [end]\n","-\n","We are here.\n","[start] අපි මෙතන [end]\n","-\n","What is your emergency?\n","[start] ඔබේ හදිසි අංකය කුමක්ද [end]\n","-\n","Who thinks so?\n","[start] ඔච්චර හිතන්න කවුද [end]\n"]}]}]}